{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part (A) Implementation of Naive Bayes Theorem\n",
    "#Que1(a),Que1(b),Que1(c)\n",
    "Run command : python Q1.py training_file_path testing_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries here\n",
    "import pandas as pd\n",
    "import re \n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from random import *\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = sys.argv[1] #training file\n",
    "# p2 = sys.argv[2] # testing file\n",
    "p1 = \"trainingandtestdata/training.1600000.processed.noemoticon.csv\" #training file\n",
    "p2 = \"trainingandtestdata/testdata.manual.2009.06.14.csv\"  #testing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data from the given data file into data frames\n",
    "#0th (label) and 5th(tweets) are only extracted from the file\n",
    "def read_data(file_path):\n",
    "    df = pd.read_csv(file_path,encoding='latin-1', usecols=[0,5], header=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = read_data(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helping function to create a dictionary\n",
    "#no_samples: how many records we need from the data file\n",
    "\n",
    "def create_dict(df_input, no_samples):\n",
    "    new_dict = {}\n",
    "    for tweet in tqdm(df_input):\n",
    "        tweet_list = tweet.split()\n",
    "        for word in tweet_list:\n",
    "            if(word in new_dict.keys()):\n",
    "                new_dict[word]+=1\n",
    "            else:\n",
    "                new_dict[word]=1\n",
    "    return new_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make two dictionaries one for each class 0 and 1\n",
    "#it puts both dictionaries into a list and returns the list\n",
    "#0 = negative, 4 = positive\n",
    "def make_dictionaries():\n",
    "    df_neg_data = df_data[df_data[0] == 0][5].tolist()\n",
    "    df_pos_data = df_data[df_data[0] == 4][5].tolist()\n",
    "    \n",
    "    \n",
    "    prob_pos_class = (len(df_pos_data)/(len(df_neg_data) + len(df_pos_data) ))\n",
    "    prob_neg_class = (len(df_neg_data)/(len(df_neg_data) + len(df_pos_data) ))\n",
    "        \n",
    "    dict_neg_data = create_dict(df_neg_data,len(df_neg_data))\n",
    "    dict_pos_data = create_dict(df_pos_data,len(df_pos_data))\n",
    "    list_freq = [dict_neg_data,dict_pos_data]\n",
    "    return list_freq,prob_pos_class,prob_neg_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It takes a dictionary as input and returns another dictionary with probability of each word\n",
    "# no of times a word occur / total no. of words (frequencies are considered not the unique words)\n",
    "# Laplace_Smoothing is done here itself\n",
    "def calculate_probs(input_dict,X):\n",
    "    dict_prob={}\n",
    "    alpha = len(X)\n",
    "    c = 1\n",
    "    total_words = sum(input_dict.values())\n",
    "    print('alpha :',alpha,total_words)\n",
    "    for i in X:\n",
    "        val = input_dict.get(i,0)\n",
    "        dict_prob[i]=((val + c)/(total_words + c*alpha))\n",
    "        \n",
    "    return dict_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800000/800000 [00:02<00:00, 298098.55it/s]\n",
      "100%|██████████| 800000/800000 [00:02<00:00, 306902.63it/s]\n"
     ]
    }
   ],
   "source": [
    "list_freq,prob_pos_class,prob_neg_class = make_dictionaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to return the list of probability dictionaries\n",
    "def return_prob_dict():\n",
    "    X = (set(list_freq[0]) | set(list_freq[1]))\n",
    "    print('no of words :',len(X))\n",
    "    #build probabilities dictionary for negative class\n",
    "    dict_neg_prob = calculate_probs(list_freq[0],X)\n",
    "    #build probabilities dictionary for positive class\n",
    "    dict_pos_prob = calculate_probs(list_freq[1],X)\n",
    "    list_prob = [dict_neg_prob, dict_pos_prob]\n",
    "    \n",
    "    return list_prob\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of words : 1350598\n",
      "alpha : 1350598 10865587\n",
      "alpha : 1350598 10216254\n"
     ]
    }
   ],
   "source": [
    "#list of probabilities class wise\n",
    "list_prob= return_prob_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1350598"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = (set(list_prob[0]) | set(list_prob[1]))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the list of words of a sentence\n",
    "def return_tokens(tweet):\n",
    "    list_of_words = tweet.split()\n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates P(new_tweet/-ve)\n",
    "def cal_prob_neg_class(new_tweet):\n",
    "    prob = 0;\n",
    "    total_keys = len(list_prob[0])\n",
    "    c = 1\n",
    "    list_of_words = return_tokens(new_tweet)\n",
    "    for word in list_of_words:\n",
    "        prob += math.log(list_prob[0].get(word,1/(c*total_keys)))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates P(new_tweet/+ve)\n",
    "def cal_prob_pos_class(new_tweet):\n",
    "    prob = 0;\n",
    "    total_keys = len(list_prob[1])\n",
    "    list_of_words = return_tokens(new_tweet)\n",
    "    c = 1\n",
    "    for word in list_of_words:\n",
    "        prob += math.log(list_prob[1].get(word,(c/(c*total_keys))))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#p(+ve/New_Tweet) = p(New_Tweet/+Ve)*p(+ve)\n",
    "#p(-ve/New_Tweet) = p(New_Tweet/-Ve)*p(+ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the class of new tweet\n",
    "def make_prediction(new_tweet):\n",
    "    prob_new_tweet_given_pos_class = cal_prob_pos_class(new_tweet)\n",
    "    prob_new_tweet_given_neg_class = cal_prob_neg_class(new_tweet)\n",
    "    \n",
    "    pos_class_prob = prob_new_tweet_given_pos_class + math.log(prob_pos_class)\n",
    "    neg_class_prob = prob_new_tweet_given_neg_class + math.log(prob_neg_class)\n",
    "    \n",
    "    if pos_class_prob > neg_class_prob:\n",
    "        return 4\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(file_path,data_set):\n",
    "    pred_list=[]\n",
    "    if data_set == 'test':\n",
    "        df_test_data = pd.read_csv(file_path,encoding='latin-1', usecols=[0,5], header=None)\n",
    "    else :\n",
    "        df_test_data = df_data\n",
    "    test_data = df_test_data[df_test_data[0] !=2]\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "    X = test_data[5].tolist()\n",
    "    for i in tqdm(X):\n",
    "        pred=make_prediction(i)\n",
    "        pred_list.append(pred)\n",
    "    test_data['pred']=pred_list\n",
    "    conf_matrix = confusion_matrix(test_data[0],test_data['pred'])\n",
    "    accuracy = len(test_data[test_data[0]==test_data['pred']])/len(test_data)\n",
    "    return accuracy,conf_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que1 (a) Report accuracy over the training as well as the test set\n"
     ]
    }
   ],
   "source": [
    "#Que1(a) Report accuracy over the training as well as the test set\n",
    "print('--------------------Que1 (a) Accuracies over training and test data--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:00<00:00, 62794.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data : 0.8161559888579387\n",
      "[[147  30]\n",
      " [ 36 146]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy,conf_matrix = calculate_accuracy(p2,'test')\n",
    "print('Accuracy on Test Data :',accuracy)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600000/1600000 [00:19<00:00, 81420.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data : 0.862645625\n",
      "[[721711  78289]\n",
      " [141478 658522]]\n"
     ]
    }
   ],
   "source": [
    "accuracy_train,conf_matrix_train = calculate_accuracy(p1,'train')\n",
    "print('Accuracy on Training Data :',accuracy_train)\n",
    "print(conf_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------Que1 (c) Confusion Matrix--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "binary = conf_matrix\n",
    "fig, ax = plot_confusion_matrix(conf_mat=binary,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rand_accuracy(file_path):\n",
    "    rand_list=[]\n",
    "    df_test_data = pd.read_csv(file_path,encoding='latin-1', usecols=[0,5], header=None)\n",
    "    test_data = df_test_data[df_test_data[0] !=2]\n",
    "    print(\"Number of examples for testing :\",len(test_data))\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "    X = test_data[5]\n",
    "    for j in range (len(test_data)):\n",
    "        rand_list.append(sample([0,4],  1)[0] )\n",
    "    \n",
    "    test_data['rand']=rand_list\n",
    "#     print(test_data)\n",
    "    print('Number of value match :',len(test_data[test_data[0] == test_data['rand']]))\n",
    "    return len(test_data[test_data[0] == test_data['rand']])/len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples for testing : 359\n",
      "Number of value match : 171\n",
      "Random classifier accuracy on test data : 0.4763231197771588\n"
     ]
    }
   ],
   "source": [
    "print('--------------------Que1 (b) Random/Majority Classifier--------------------')\n",
    "print('Random classifier accuracy on test data :',calculate_rand_accuracy(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_major_accuracy(file_path,val):\n",
    "    major_list=[]\n",
    "    df_test_data = pd.read_csv(file_path,encoding='latin-1', usecols=[0,5], header=None)\n",
    "    test_data = df_test_data[df_test_data[0] !=2]\n",
    "    print(\"Number examples for testing :\",len(test_data))\n",
    "    test_data.reset_index(drop=True, inplace=True)\n",
    "    X = test_data[5]\n",
    "    for j in range (len(test_data)):\n",
    "        major_list.append(val)\n",
    "    \n",
    "    test_data['major']=major_list\n",
    "    print('Number of value match :',len(test_data[test_data[0] == test_data['major']]))\n",
    "    return len(test_data[test_data[0] == test_data['major']])/len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number examples for testing : 359\n",
      "Number of value match : 177\n",
      "Accuracy of majority classifier on test data as 0 as majority value : 0.49303621169916434\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of majority classifier on test data as 0 as majority value :',calculate_major_accuracy(p2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number examples for testing : 359\n",
      "Number of value match : 182\n",
      "Accuracy of majority classifier on test data as 0 as majority value : 0.5069637883008357\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of majority classifier on test data as 0 as majority value :',calculate_major_accuracy(p2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
